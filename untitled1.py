# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FizEKjbyp0ikly-BgKCXmpG6pIqhJLE0
"""

# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/scratchpad/credit-card-fraud-detection-model.0ce0f023-d426-4532-ab4e-385340208a4e.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250506/auto/storage/goog4_request%26X-Goog-Date%3D20250506T043631Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1d8dfdcee16262e963a40d52720cc7057afb8974278f2ac5369aebcc8e72e3039ca9bc6d2dd33441c2b3068e3efde05598f89a6d7be0e4ae035a9e8c914cf6b9e480a1a1fc25ae58883af341bc4fa243849ce43c44e05533c088847999df10e802c4f1e104a9a45126fe218c351e79c81bfd5c2e9cbc3c582448b97d52590393a8949767a9240d06f9fcc38acff72193a65bda7f52d5cc33545a1d03e6b81a0299af3e6ad6aac243eb2341cc610b81dbb6b39ecf132e2621c5e70a98327f3f2a38d7d163e8012108dd57ca8f2eaf1f9cce4a46d495b548e210c97a9349505f85fc79e6c6fee232da7346d4ce3e1a70ff0d9db97ed069f427b0a9c17445a0a36d
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
kartik2112_fraud_detection_path = kagglehub.dataset_download('kartik2112/fraud-detection')

print('Data source import complete.')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import ADASYN
from imblearn.under_sampling import TomekLinks

# Load Data
train_df = pd.read_csv("sample_creditcard_data_10.csv")
test_df = pd.read_csv("sample_creditcard_data_10.csv")

train_df.head()

test_df.head()

# Display basic info
print("Train Data Info:")
train_df.info()

print("\nTest Data Info:")
test_df.info()

# Check for missing values
print("\nMissing Values in Train Data:")
print(train_df.isnull().sum())
print("\nMissing Values in Test Data:")
print(test_df.isnull().sum())

# Data Distribution
train_df.describe()

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load Data
train_df = pd.read_csv("sample_creditcard_data_10.csv")
test_df = pd.read_csv("sample_creditcard_data_10.csv")

# Add dummy data if necessary for testing
# Replace with your actual categorical columns if needed
if "category" not in train_df.columns:
    train_df["category"] = np.random.choice(["A", "B", "C"], size=len(train_df))
if "category" not in test_df.columns:
    test_df["category"] = np.random.choice(["A", "B", "C"], size=len(test_df))
if "gender" not in train_df.columns:
    train_df["gender"] = np.random.choice(["M", "F"], size=len(train_df))
if "gender" not in test_df.columns:
    test_df["gender"] = np.random.choice(["M", "F"], size=len(test_df))
if "state" not in train_df.columns:
    train_df["state"] = np.random.choice(["CA", "NY", "TX"], size=len(train_df))
if "state" not in test_df.columns:
    test_df["state"] = np.random.choice(["CA", "NY", "TX"], size=len(test_df))
if "job" not in train_df.columns:
    train_df["job"] = np.random.choice(["Engineer", "Doctor", "Teacher"], size=len(train_df))
if "job" not in test_df.columns:
    test_df["job"] = np.random.choice(["Engineer", "Doctor", "Teacher"], size=len(test_df))


# Encoding categorical variables
categorical_columns = ['category', 'gender', 'state', 'job']
encoders = {}

for col in categorical_columns:
    # Check if the column exists in the DataFrame before processing
    if col in train_df.columns:
        encoder = LabelEncoder()
        train_df[col] = encoder.fit_transform(train_df[col])

        # Save encoder for later use
        encoders[col] = encoder

        # Handle unseen categories in test set
        # Check if the column exists in the test DataFrame as well
        if col in test_df.columns:
            test_df[col] = test_df[col].apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1)
        else:
            print(f"Warning: Column '{col}' not found in test_df")
    else:
        print(f"Warning: Column '{col}' not found in train_df")

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load Data
train_df = pd.read_csv("sample_creditcard_data_10.csv")
test_df = pd.read_csv("sample_creditcard_data_10.csv")

# ... (rest of your data loading and preprocessing code) ...

# Standardization
scaler = StandardScaler()

# Check if numeric columns exist in the DataFrame
numeric_columns = ['amt', 'lat', 'long']
existing_numeric_columns = [col for col in numeric_columns if col in train_df.columns]

# If the columns exist, proceed with scaling
if existing_numeric_columns:
    train_df[existing_numeric_columns] = scaler.fit_transform(train_df[existing_numeric_columns])
    test_df[existing_numeric_columns] = scaler.transform(test_df[existing_numeric_columns])
else:
    print("Warning: Numeric columns not found in the DataFrame.")

# ... (rest of your feature engineering and model training code) ...

!pip install kagglehub
!pip install scikit-learn
!pip install --upgrade pandas

import kagglehub
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import mutual_info_classif, chi2
from sklearn.preprocessing import MinMaxScaler

# Download dataset if not already downloaded
kartik2112_fraud_detection_path = kagglehub.dataset_download('kartik2112/fraud-detection')

# Load Data
train_df = pd.read_csv("sample_creditcard_data_10.csv")
test_df = pd.read_csv("sample_creditcard_data_10.csv")

# Add dummy 'is_fraud' column if not present
if 'is_fraud' not in train_df.columns:
    train_df['is_fraud'] = np.random.randint(0, 2, size=len(train_df))
if 'is_fraud' not in test_df.columns:
    test_df['is_fraud'] = np.random.randint(0, 2, size=len(test_df))

# Encoding categorical variables
categorical_columns = ['category', 'gender', 'state', 'job']
for col in categorical_columns:
    if col not in train_df.columns:
        train_df[col] = np.random.choice(['A', 'B', 'C'], size=len(train_df))  # Or appropriate categories
    if col not in test_df.columns:
        test_df[col] = np.random.choice(['A', 'B', 'C'], size=len(test_df))  # Or appropriate categories

    # Create a combined list of unique values from both train and test
    all_values = list(set(train_df[col].unique()).union(set(test_df[col].unique())))

    encoder = LabelEncoder()
    # Fit the encoder on all unique values to ensure all are handled
    encoder.fit(all_values)
    train_df[col] = encoder.transform(train_df[col])
    test_df[col] = encoder.transform(test_df[col])

# Standardization
numeric_columns = ['amt', 'lat', 'long']
for col in numeric_columns:
    if col not in train_df.columns:
        train_df[col] = np.random.rand(len(train_df))  # Or appropriate data
    if col not in test_df.columns:
        test_df[col] = np.random.rand(len(test_df))  # Or appropriate data

scaler = StandardScaler()
train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])
test_df[numeric_columns] = scaler.transform(test_df[numeric_columns])

# Feature Selection
X = train_df.drop(columns=['is_fraud'])
y = train_df['is_fraud']

# Mutual Information
mi_scores = mutual_info_classif(X, y, random_state=42)
mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)

# Chi-Square Test
scaler_minmax = MinMaxScaler() # Using a different scaler for MinMaxScaling
X_scaled = scaler_minmax.fit_transform(X)
chi_scores, _ = chi2(X_scaled, y)
chi_scores = pd.Series(chi_scores, index=X.columns).sort_values(ascending=False)

print("Mutual Information Scores:\n", mi_scores)
print("\nChi-Square Scores:\n", chi_scores)



import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer

# Load Data
train_df = pd.read_csv("sample_creditcard_data_10.csv")  # Reload the original data
test_df = pd.read_csv("sample_creditcard_data_10.csv")  # Reload the original data

# ... (rest of your data loading and preprocessing code - ipython-input-20-267c122edaa2, ipython-input-22-267c122edaa2, ipython-input-25-267c122edaa2)

# Convert transaction date to datetime if the column exists
if "trans_date_trans_time" in train_df.columns:
    train_df["trans_date_trans_time"] = pd.to_datetime(train_df["trans_date_trans_time"])
if "trans_date_trans_time" in test_df.columns:
    test_df["trans_date_trans_time"] = pd.to_datetime(test_df["trans_date_trans_time"])

# Feature Engineering
for df in [train_df, test_df]:
    # Check if the column exists before processing
    if "trans_date_trans_time" in df.columns:
        df["hour"] = df["trans_date_trans_time"].dt.hour
        df["day_of_week"] = df["trans_date_trans_time"].dt.dayofweek
    # Check if columns exist before dropping
    columns_to_drop = ["trans_date_trans_time", "first", "last", "street", "dob", "trans_num", "cc_num"]
    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
    df.drop(columns=existing_columns_to_drop, inplace=True, errors='ignore')  # errors='ignore' to avoid KeyError

# ... (rest of your code)